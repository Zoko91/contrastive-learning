{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Testing different representations and their respective accuracies on CIFAR10"
      ],
      "metadata": {
        "id": "Gmkv6qaYDOZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torch torchvision"
      ],
      "metadata": {
        "id": "ZvP0ZxKXBUER"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline MLP"
      ],
      "metadata": {
        "id": "aWjlDa0XDNTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.functional import cosine_similarity\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "3IT85Tb8BPV7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DMKWMUt_A_sd"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Define transformation to convert CIFAR-10 images to grayscale\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_dataset_mlp = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset_mlp = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define data loaders\n",
        "batch_size = 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmFImQ-WBEbw",
        "outputId": "a41b5ac1-071d-4b6b-b47b-41fb4ab4c8ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation\n",
        "def get_transforms(means, stds):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(means, stds),\n",
        "    ])\n",
        "    return transform\n",
        "\n",
        "# Calculate the mean and std of the subset dataset\n",
        "def get_mean_std(dataset):\n",
        "    \"\"\"Compute the mean and std value of dataset.\"\"\"\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    print('==> Computing mean and std..')\n",
        "    for inputs, targets in dataloader:\n",
        "        for i in range(3):\n",
        "            mean[i] += inputs[:, i, :, :].mean()\n",
        "            std[i] += inputs[:, i, :, :].std()\n",
        "    mean.div_(len(dataset))\n",
        "    std.div_(len(dataset))\n",
        "    return mean, std"
      ],
      "metadata": {
        "id": "vGFjDhXnBkqF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean and std of the subset dataset\n",
        "train_means_mlp, train_stds_mlp = get_mean_std(train_dataset_mlp)\n",
        "print(f'Means: {train_means_mlp}')\n",
        "print(f'Standard deviations: {train_stds_mlp}\\n')\n",
        "val_means_mlp, val_stds_mlp = get_mean_std(test_dataset_mlp)\n",
        "print(f'Validation means: {val_means_mlp}')\n",
        "print(f'Validation standard deviations: {val_stds_mlp}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgNun4JdBKHX",
        "outputId": "b147e549-ad60-4883-8bcb-73e063d4d33c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Computing mean and std..\n",
            "Means: tensor([0.4914, 0.4822, 0.4465])\n",
            "Standard deviations: tensor([0.1953, 0.1925, 0.1942])\n",
            "\n",
            "==> Computing mean and std..\n",
            "Validation means: tensor([0.4942, 0.4851, 0.4504])\n",
            "Validation standard deviations: tensor([0.1949, 0.1922, 0.1944])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "train_dataset_mlp = datasets.CIFAR10(root='./data', train=True, download=True, transform=get_transforms(train_means_mlp,train_stds_mlp))\n",
        "val_dataset_mlp = datasets.CIFAR10(root='./data', train=False, download=True, transform=get_transforms(val_means_mlp,val_stds_mlp))\n",
        "\n",
        "# Data Loader\n",
        "train_loader_mlp = DataLoader(train_dataset_mlp, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader_mlp = DataLoader(val_dataset_mlp, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "bfot3BFhBLdf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "input_size = 224 * 224 * 3  # Size of CIFAR-10 images after flattening\n",
        "hidden_size = 256\n",
        "output_size = 10  # Number of classes in CIFAR-10\n",
        "mlp_model = MLP(input_size, hidden_size, output_size)\n",
        "mlp_model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "CMdKmW2MBMal"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    mlp_model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for i, data in enumerate(tqdm(train_loader_mlp, desc=f'Epoch {epoch + 1}/{num_epochs}')):\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = mlp_model(images.view(images.size(0), -1))\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    average_loss = total_loss / len(train_loader_mlp)\n",
        "    accuracy = correct / total_samples\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}, Accuracy: {accuracy}')\n",
        "\n",
        "print('Training finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e29tR0WZBNT8",
        "outputId": "42bcf215-93eb-41bb-c13d-5bcab807d7bd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 391/391 [01:54<00:00,  3.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 17.21063083212089, Accuracy: 0.30888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 391/391 [01:37<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 3.088271303859818, Accuracy: 0.40726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 391/391 [01:50<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Loss: 1.7567208654740278, Accuracy: 0.45616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 391/391 [01:53<00:00,  3.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Loss: 1.6633597334937367, Accuracy: 0.46956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 391/391 [01:37<00:00,  4.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Loss: 1.7166288804520122, Accuracy: 0.46342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 391/391 [01:30<00:00,  4.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Loss: 1.8302981871778092, Accuracy: 0.4515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 391/391 [01:31<00:00,  4.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Loss: 1.9657779968608067, Accuracy: 0.43902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 391/391 [01:30<00:00,  4.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Loss: 2.0605919226965943, Accuracy: 0.4319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 391/391 [01:30<00:00,  4.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Loss: 2.1912501605270465, Accuracy: 0.42092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 391/391 [01:29<00:00,  4.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 2.3590534202887885, Accuracy: 0.41556\n",
            "Training finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_model.eval()\n",
        "correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in tqdm(test_loader_mlp, desc='Testing'):\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = mlp_model(images.view(images.size(0), -1))\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "accuracy = correct / total_samples\n",
        "print(f'Test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "825-DNyKBOlb",
        "outputId": "c4f06317-f3ea-49b3-b22d-299c7aed3785"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 79/79 [00:17<00:00,  4.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.3577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading CIFAR10 again"
      ],
      "metadata": {
        "id": "_amrnoAJDLNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "RHeZ36tWDUoK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from torch.utils.data import TensorDataset"
      ],
      "metadata": {
        "id": "-Bb1WJf-DTw2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Fetch CIFAR-10 data\n",
        "cifar10 = fetch_openml('CIFAR_10_small', version=1)\n",
        "\n",
        "# Extract images and labels\n",
        "images = cifar10.data\n",
        "labels = cifar10.target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQWRBft3Dak_",
        "outputId": "9abb0e68-429b-4e87-afb2-6cf5e76a3e69"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "    images, labels, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "v0fdZkK7Jpgf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=512)  # You can adjust the number of components\n",
        "train_images_pca = pca.fit_transform(train_images)\n",
        "test_images_pca = pca.transform(test_images)"
      ],
      "metadata": {
        "id": "g-P62OySKaBU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply t-SNE (Optional: First reduce dimensionality with PCA for faster processing)\n",
        "tsne = TSNE(n_components=3, perplexity=50.0, n_iter=1000)\n",
        "train_images_tsne = tsne.fit_transform(pca.transform(train_images))  # Using PCA reduced data\n",
        "test_images_tsne = tsne.fit_transform(pca.transform(test_images))    # Using PCA reduced data"
      ],
      "metadata": {
        "id": "U1cadaqDNXkF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply t-SNE (Optional: First reduce dimensionality withOUT PCA)\n",
        "tsne = TSNE(n_components=3, perplexity=50.0, n_iter=1000)\n",
        "train_images_tsne = tsne.fit_transform(train_images)\n",
        "test_images_tsne = tsne.fit_transform(test_images)"
      ],
      "metadata": {
        "id": "Kfz7W5jKQVBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "\n",
        "# Convert pandas Series to numpy arrays\n",
        "train_labels_np = train_labels.astype(int).to_numpy()\n",
        "test_labels_np = test_labels.astype(int).to_numpy()\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_dataset_pca = TensorDataset(torch.tensor(train_images_pca, dtype=torch.float32), torch.tensor(train_labels_np, dtype=torch.long))\n",
        "test_dataset_pca = TensorDataset(torch.tensor(test_images_pca, dtype=torch.float32), torch.tensor(test_labels_np, dtype=torch.long))\n",
        "\n",
        "train_loader_pca = DataLoader(train_dataset_pca, batch_size=128, shuffle=True)\n",
        "test_loader_pca = DataLoader(test_dataset_pca, batch_size=128, shuffle=False)\n",
        "\n",
        "# Repeat for t-SNE data if you plan to use it\n",
        "train_dataset_tsne = TensorDataset(torch.tensor(train_images_tsne, dtype=torch.float32), torch.tensor(train_labels_np, dtype=torch.long))\n",
        "test_dataset_tsne = TensorDataset(torch.tensor(test_images_tsne, dtype=torch.float32), torch.tensor(test_labels_np, dtype=torch.long))\n",
        "\n",
        "train_loader_tsne = DataLoader(train_dataset_tsne, batch_size=128, shuffle=True)\n",
        "test_loader_tsne = DataLoader(test_dataset_tsne, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "ari5UhCIKe7W"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        # Move data to device (CPU or GPU)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        total_samples += target.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total_samples\n",
        "    return avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "NrI-TxiNFDVR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            # Move data to device\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "\n",
        "            # Compute loss\n",
        "            total_loss += criterion(output, target).item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            total_samples += target.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total_samples\n",
        "    return avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "GClg7RYzFE4Q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA pipeline"
      ],
      "metadata": {
        "id": "f-AFg-nKEUKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the MLP model for PCA\n",
        "input_size_pca = 512\n",
        "mlp_model_pca = MLP(input_size_pca, hidden_size, output_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mlp_model_pca.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "print('PCA Experiment\\n\\n')\n",
        "num_epochs = 10\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    train_loss, train_accuracy = train_model(mlp_model_pca, train_loader_pca, optimizer, criterion, device)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%')\n",
        "\n",
        "# Evaluating the model\n",
        "test_loss, test_accuracy = evaluate_model(mlp_model_pca, test_loader_pca, criterion, device)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4ytXSpgFLzx",
        "outputId": "7dd7b635-09a6-46b3-9765-1ac94b939d38"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA Experiment\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:00<00:07,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 15.7483, Train Acc: 31.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:01<00:04,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Train Loss: 5.3906, Train Acc: 48.81%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:01<00:03,  2.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Train Loss: 2.8565, Train Acc: 59.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:01<00:02,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Train Loss: 1.8171, Train Acc: 66.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:02<00:01,  2.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Train Loss: 1.2411, Train Acc: 72.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [00:02<00:01,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Train Loss: 0.9269, Train Acc: 77.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [00:02<00:01,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Train Loss: 0.7175, Train Acc: 80.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [00:03<00:00,  3.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Train Loss: 0.5974, Train Acc: 83.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [00:03<00:00,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Train Loss: 0.6479, Train Acc: 82.92%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:03<00:00,  2.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Train Loss: 0.5797, Train Acc: 84.78%\n",
            "Test Loss: 6.1763, Test Acc: 39.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "T-sne pipeline"
      ],
      "metadata": {
        "id": "eeYh9a6PEdqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the MLP model for Tsne\n",
        "input_size_tsne = 3\n",
        "mlp_model_tsne = MLP(input_size_tsne, hidden_size, output_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_tsne = optim.Adam(mlp_model_tsne.parameters(), lr=0.01)\n",
        "\n",
        "# Training the model\n",
        "print('T-sne Experiment\\n\\n')\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train_model(mlp_model_tsne, train_loader_tsne, optimizer_tsne, criterion, device)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "uQbSPdn_FNTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data for plotting\n",
        "models = ['Images', 'CLR embeddings', 'PCA', 'T-sne']\n",
        "accuracies = [0.3577, 0.4863, 0.3967, 0.2872]\n",
        "\n",
        "# Set up the bar colors\n",
        "colors = ['skyblue', 'seagreen', 'salmon', 'plum']\n",
        "\n",
        "# Plotting the bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(models, accuracies, color=colors)\n",
        "\n",
        "# Add the data labels on top of the bars\n",
        "for bar, accuracy in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{accuracy:.4f}',\n",
        "             ha='center', va='bottom')\n",
        "\n",
        "# Title and labels\n",
        "plt.title('Test Accuracy for our MLP model with different inputs')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "\n",
        "# Save the plot as a file and display\n",
        "file_path = 'model_accuracies_bar_chart.png'\n",
        "plt.tight_layout()\n",
        "plt.savefig(file_path)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "16FKdOA7WKWq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}